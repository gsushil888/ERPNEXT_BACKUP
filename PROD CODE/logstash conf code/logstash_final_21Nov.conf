input {
  beats {
    port => 5044
  }
  beats {
    port => 5045
  }
  http_poller {
    urls => {
      elasticsearch_query => {
        method => "get"
        url => "http://10.100.0.199:9200/synthetic_index/_search"
        body => '{"size": 0, "query": {"bool": {"must": [{"term": {"url.keyword": "https://drerpnext.jfs.in/"}}, {"range": {"@timestamp": {"gte": "now-10m/m", "lte": "now"}}}]}}, "aggs": {"status_count": {"terms": {"field": "email_status.keyword", "size": 2}}}}'
        headers => {
          "Content-Type" => "application/json"
        }
        auth => {
          user => "elastic"
          password => "Clover@123"
        }
      }
    }
    request_timeout => 60
    #schedule => { cron => "* * * * *" }  # every minute
    schedule => { cron => "*/1 * * * *" }
    codec => "json"
    tags => ["uptime_log","dr"]
  }
#  http_poller {
#    urls => {
#      elasticsearch_query => {
#        method => "get"
#        url => "http://10.100.0.199:9200/synthetic_index/_search"
#        body => '{"size": 0, "query": {"bool": {"must": [{"term": {"url.keyword": "https://erpnext.jfs.in/"}}, {"range": {"@timestamp":
# {"gte": "now-10m/m", "lte": "now"}}}]}}, "aggs": {"status_count": {"terms": {"field": "email_status.keyword", "size": 2}}}}'
#        headers => {
#          "Content-Type" => "application/json"
#        }
#        auth => {
#          user => "elastic"
#          password => "Clover@123"
#        }
#      }
#    }
#    request_timeout => 60
#    #schedule => { cron => "* * * * *" }  # every minute
#    schedule => { cron => "*/1 * * * *" }
#    codec => "json"
#    tags => ["uptime_log","dc"]
#  }

}
filter {
  if "uptime_log" in [tags] {
    mutate {
      add_field => { "[type]" => "uptime-log" }
    }
    # Extract total document count
    mutate {
      add_field => {
        "total_count" => "%{[hits][total][value]}"
      }
    }
    # Extract email status counts from the aggregation results
    ruby {
      code => '
        agg_buckets = event.get("[aggregations][status_count][buckets]")
        na_count = 0
        success_count = 0

        if agg_buckets
          agg_buckets.each do |bucket|
            if bucket["key"] == "N/A"
              na_count = bucket["doc_count"]
            else
              success_count += bucket["doc_count"]
            end
          end
        end

        total_count = event.get("total_count").to_f
        uptime_percentage = total_count > 0 ? ((na_count) / total_count) * 100 : 0

        event.set("status_NA_count", na_count)
        event.set("status_success_count", success_count)
        event.set("uptime_percentage", uptime_percentage)
      '
    }
    # Limit uptime_percentage to 2 decimal places and add "%" symbol
    ruby {
      code => '
        percentage = event.get("uptime_percentage").to_f
        percentage = "%.2f" % percentage
        event.set("percentage", "#{percentage}%")
      '
    }
    # Set the URL as a field
    if "dc" in [tags]{
      mutate {
        add_field => {
            "[url]" => "https://erpnext.jfs.in/"
            "[environment]" => "DC" 
          }
      }
    }
    if "dr" in [tags]{
      mutate {
        add_field => { 
            "[url]" => "https://drerpnext.jfs.in/"
            "[environment]" => "DR"
          }
      }
    }
    # Remove unnecessary fields
    mutate {
      remove_field => ["hits", "_shards", "took", "timed_out", "aggregations", "event", "@version"]
    }
  }
  if [agent][type] == "metricbeat"{
    mutate {
       uppercase => ["[host][hostname]"]
    }
    mutate {
        add_field => { "[type]" => "metricbeat_log" }
    }
    mutate {
        remove_field => ["[@version]","[agent][ephemeral_id]","[agent][id]","[agent][version]","[event]","[host][architecture]", "[system][process]","[system][filesystem]","[system][fsstat]","[system][network]", "[host][id]", "[host][ip]","[host][mac]","[host][os][build]", "[host][os][kernel]","[host][os][version]","[ecs]","[service]","[metricset]"]
    }
  }
  if [agent][type] == "heartbeat" {
    mutate {
      add_field => { "host_name" => "%{[url][domain]}" }
    }
    if [url][domain] == "10.100.23.226" {
      mutate {
        add_field => { 
          "[servername]" => "PRODWEB"
          "[category]" => "ERPNEXT" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.100.22.142" {
      mutate {
        add_field => { 
          "[servername]" => "PRODDB"
          "[category]" => "ERPNEXT" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.100.21.216" {
      mutate {
        add_field => { 
          "[servername]" => "PRODAPP"
          "[category]" => "ERPNEXT" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.200.43.223" {
      mutate {
        add_field => { 
          "[servername]" => "DRWEB"
          "[category]" => "ERPNEXT" 
          "[environment]" => "DR"
        }
      }
    }
    if [url][domain] == "10.200.42.66" {
      mutate {
        add_field => { 
          "[servername]" => "DRDB"
          "[category]" => "ERPNEXT" 
          "[environment]" => "DR"
        }
      }
    }
    if [url][domain] == "10.200.41.138" {
      mutate {
        add_field => { 
          "[servername]" => "DRAPP"
          "[category]" => "ERPNEXT" 
          "[environment]" => "DR"
        }
      }
    }
    if [url][domain] == "10.100.27.191" {
      mutate {
        add_field => { 
          "[servername]" => "PRODJIOBLKWMCAPP"
          "[category]" => "BLACKROCK_WMC" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.100.28.180" {
      mutate {
        add_field => { 
          "[servername]" => "PRODJIOBLKWMCDB"
          "[category]" => "BLACKROCK_WMC" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.100.29.213" {
      mutate {
        add_field => { 
          "[servername]" => "PRODJIOBLKWMCPROXY"
          "[category]" => "BLACKROCK_WMC" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.100.27.52" {
      mutate {
        add_field => { 
          "[servername]" => "PRODJIOBLKAMCAPP" 
          "[category]" => "BLACKROCK_AMC" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.100.28.19" {
      mutate {
        add_field => { 
          "[servername]" => "PRODJIOBLKAMCDB" 
          "[category]" => "BLACKROCK_AMC" 
          "[environment]" => "DC"
        }
      }
    }
    if [url][domain] == "10.100.29.115" {
      mutate {
        add_field => { 
          "[servername]" => "PRODJIOBLKAMCPROXY" 
          "[category]" => "BLACKROCK_AMC" 
          "[environment]" => "DC"
        }
      }
    }
    if ![icmp][rtt][us] and ![resolve][rtt][us] {
        mutate {
            add_field => { "[latency]" => 10000 }
        }
    }
    else {
        ruby {
            code => '
                icmp_rtt_us = event.get("[icmp][rtt][us]").to_f
                event.set("[latency]", icmp_rtt_us / 1000.0)
            '
        }
    }
    mutate {
        add_field => { "[type]" => "heartbeat_log" }
    }
    mutate {
        remove_field => ["[@version]","[agent][ephemeral_id]","[agent][id]","[agent][version]","[event][duration]","[host][architecture]", "[host][cpu][usage]", "[host][id]", "[host][ip]","[host][mac]","[host][os][build]", "[host][os][kernel]","[host][os][version]"]
    }
    mutate{
        remove_field => ["observer", "ecs", "event","[monitor]","[summary]","[state]","[error]","tags","[icmp][requests]"]
    }
  }
  if [app] == "app-frappe-log" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} %{WORD:module} %{GREEDYDATA:log_message}\s+Site: %{WORD:site_name}\s+Form Dict: %{GREEDYDATA:payload_json}"
      }
    }
#    date {
#      match => [ "log_timestamp", "YYYY-MM-dd HH:mm:ss,SSS" ]
#      target => "@timestamp"
#      timezone => "Asia/Kolkata"
#    }
    mutate {
      add_field => { "formatted_log_timestamp" => "%{log_timestamp}" }
    }
    ruby {
      code => "
        # Parse the original log_timestamp string into a DateTime object
        original_timestamp = event.get('formatted_log_timestamp')
        require 'time'  # Require the time library for parsing
        parsed_time = Time.strptime(original_timestamp, '%Y-%m-%d %H:%M:%S,%L')  # Parse the original format
        # Format it to dd-MMM-YYYY hh:mm:ss AM/PM
        formatted_time = parsed_time.strftime('%d-%b-%Y %I:%M:%S %p')
        event.set('formatted_log_timestamp', formatted_time)  # Set the formatted time
      "
    }
    mutate {
      remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]"]
    }
  }
  if [app] == "dr-sync-log" {
    grok {
      match => { 
        "message" => "%{SYSLOGTIMESTAMP:log_timestamp} %{HOSTNAME:hostname} CRON\[%{NUMBER:pid}\]: \(%{USER:user}\) CMD \(%{GREEDYDATA:command}\)"
      }
    }
    if !(" /home/frappe/frappe-bench/apps/" in [command]) {
      drop { }
    }
    date {
        match => [ "log_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]  # Adjust to match the format of your log
        target => "@timestamp"
    }
    mutate {
        add_field => { "formatted_log_timestamp" => "%{log_timestamp}" }
    }
    ruby {
        code => "
            original_timestamp = event.get('formatted_log_timestamp')
            require 'time'
            # Parse the original log_timestamp string into a DateTime object
            parsed_time = Time.strptime(original_timestamp, '%b %d %H:%M:%S')  # Adjust according to log format
            # Format it to dd-MMM-YYYY hh:mm:ss AM/PM
            formatted_time = parsed_time.strftime('%d-%b-%Y %I:%M:%S %p')
            event.set('formatted_log_timestamp', formatted_time)  # Set the formatted time
        "
    }
    mutate {
      remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
    }
  }
#  if [app] == "db-slow-queries-test" {
#    grok {
#        match => {
#            "message" => [
#
#                "%{DATA:path}, Version: %{DATA:version} \(%{DATA:db} binary distribution\)\. started with:\s+Tcp port: %{NUMBER:tcp_port}  Unix socket: %{DATA:unix_socket}\s+Time\s+Id\s+Command\s+Argument\s+# Time: (?<connection_time>%{DATA:time}\s+%{TIME:log_time_actual})\s+(?<connection_string># User@Host: %{DATA:user_id}\[%{DATA:user_id_inner}\] @ %{DATA:host_name}? \[%{IP:host_ip}?\])\s+# Thread_id: %{NUMBER:thread_id}\s+Schema: %{DATA:schema}\s+QC_hit: %{WORD:qc_hit}\s+# Query_time: %{NUMBER:query_time:float}\s+Lock_time: %{NUMBER:lock_time:float}\s+Rows_sent: %{NUMBER:rows_sent}\s+Rows_examined: %{NUMBER:rows_examined}\s+# Rows_affected: %{NUMBER:rows_affected}\s+Bytes_sent: %{NUMBER:bytes_sent}\s+use %{DATA:database};\s+SET timestamp=%{NUMBER:timestamp};\s+%{GREEDYDATA:query_statement}",
#
#                "%{DATA:path}, Version: %{DATA:version} \(%{DATA:db} binary distribution\)\. started with:\s+Tcp port: %{NUMBER:tcp_port}  Unix socket: %{DATA:unix_socket}\s+Time\s+Id\s+Command\s+Argument"
#            ]
#        }
#    }
    # Drop events without query_statement and query_time
#    if !([query_statement] and [query_time]) {
#        drop { }
#    }
#    mutate {
#        remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
#    }
#  }
  if [app] == "db-slow-queries-log" {
    grok {
      match => {
#         "message" => "# Time: (?<connection_time>%{DATA:time}\s+%{TIME:log_time_actual})\s+(?<connection_string># User@Host: %{DATA:user_id}\[%{DATA:user_id_inner}\] @ %{DATA:host_name}? \[%{IP:host_ip}?\])\s+# Thread_id: %{NUMBER:thread_id}\s+Schema: %{DATA:schema}\s+QC_hit: %{WORD:qc_hit}\s+# Query_time: %{NUMBER:query_time:float}\s+Lock_time: %{NUMBER:lock_time:float}\s+Rows_sent: %{NUMBER:rows_sent}\s+Rows_examined: %{NUMBER:rows_examined}\s+# Rows_affected: %{NUMBER:rows_affected}\s+Bytes_sent: %{NUMBER:bytes_sent}\s+use %{DATA:database};\s+SET timestamp=%{NUMBER:timestamp};\s+%{GREEDYDATA:query_statement};"
#         "message" => "# Time: (?<connection_time>%{DATA:time}\s+%{TIME:log_time_actual})\s+(?<connection_string># User@Host: %{DATA:user_id}\[%{DATA:user_id_inner}\] @ %{DATA:host_name}? \[%{IP:host_ip}?\])\s+# Thread_id: %{NUMBER:thread_id}\s+Schema: %{DATA:schema}\s+QC_hit: %{WORD:qc_hit}\s+# Query_time: %{NUMBER:query_time:float}\s+Lock_time: %{NUMBER:lock_time:float}\s+Rows_sent: %{NUMBER:rows_sent}\s+Rows_examined: %{NUMBER:rows_examined}\s+# Rows_affected: %{NUMBER:rows_affected}\s+Bytes_sent: %{NUMBER:bytes_sent}\s+(?:use %{DATA:database};)?\s+SET timestamp=%{NUMBER:timestamp};\s+%{GREEDYDATA:query_statement};"
          "message" => "# Time: (?<log_timestamp>%{DATA:log_date}(\s+|\\n)%{TIME:log_time})(\s+|\\n)(?<connection_string># User@Host: %{DATA:user_id}\[%{DATA:user_id_inner}\] @ %{DATA:host_name}? \[%{IP:host_ip}?\])(\s+|\\n)# Thread_id: %{NUMBER:thread_id}(\s+|\\n)Schema: %{DATA:schema}(\s+|\\n)QC_hit: %{WORD:qc_hit}(\s+|\\n)# Query_time: %{NUMBER:query_time:float}(\s+|\\n)Lock_time: %{NUMBER:lock_time:float}(\s+|\\n)Rows_sent: %{NUMBER:rows_sent}(\s+|\\n)Rows_examined: %{NUMBER:rows_examined}(\s+|\\n)# Rows_affected: %{NUMBER:rows_affected}(\s+|\\n)Bytes_sent: %{NUMBER:bytes_sent}(?:(\s+|\\n)use %{DATA:database};)?(\s+|\\n)SET timestamp=%{NUMBER:unix_timestamp};(\s+|\\n)%{GREEDYDATA:query_statement};"
      }
    }
    # Drop events without query_statement and query_time
#    if !([query_statement] and [query_time]) {
#        drop { }
#    }
    date {
        match => [ "log_timestamp", "yyMMdd HH:mm:ss","yyMMdd  H:mm:ss"  ]
        target => "@timestamp"
    }
#    ruby {
#      code => "
#        # Adjust @timestamp by 5 hours and 30 minutes (19800 seconds)
#        timestamp = event.get('@timestamp')
#        if timestamp.is_a?(LogStash::Timestamp)
#          new_timestamp = timestamp.time - 19800  # Add 5 hours 30 minutes in seconds
#          event.set('@timestamp', LogStash::Timestamp.new(new_timestamp))
#        end
#      "
#    }
    date {
        match => [ "log_timestamp", "yyMMdd HH:mm:ss","yyMMdd  H:mm:ss" ]
        target => "log_timestamp"
    }
    ruby {
        code => "
            log_timestamp = event.get('log_timestamp')
            if log_timestamp
                # Parse the log_timestamp object to Time object and format it
                formatted_time = log_timestamp.time.strftime('%d-%b-%Y %I:%M:%S %p')
                event.set('formatted_log_timestamp', formatted_time)
                                # Extract date and time separately
                log_date = log_timestamp.time.strftime('%d-%b-%Y')
                log_time = log_timestamp.time.strftime('%I:%M:%S %p')
                event.set('log_date', log_date)
                event.set('log_time', log_time)
            end
        "
    }
    mutate {
        remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
    }
  }
  if [app] == "web-error-log" { 
     grok {
       match => {
         "message" => "(?<timestamp>%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME}) \[%{LOGLEVEL:log_level}\] %{NUMBER:process_id}#%{NUMBER:process_instance}: \*%{NUMBER:request_id} %{GREEDYDATA:log_message}"
       }
       tag_on_failure => ["_grokparsefailure"]
     }
     if [log_level] == "debug" or [log_level] == "info" {
       drop { }
     }
#     date {
#       match => [ "timestamp", "YYYY/MM/dd HH:mm:ss" ]
#       target => "@timestamp"  # Set the parsed timestamp to the @timestamp field
#     }
     mutate {
      remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
     }
     if "_grokparsefailure" in [tags] {
       drop { }
     }
  }
  if [app] == "app-nginx-error-log" {
     grok {
       match => {
         "message" => "(?<timestamp>%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME}) \[%{LOGLEVEL:log_level}\] %{NUMBER:process_id}#%{NUMBER:proc
ess_instance}: \*%{NUMBER:request_id} %{GREEDYDATA:log_message}"
       }
       tag_on_failure => ["_grokparsefailure"]
     }
     if [log_level] == "debug" or [log_level] == "info" {
       drop { }
     }
#     date {
#       match => [ "timestamp", "YYYY/MM/dd HH:mm:ss" ]
#       target => "@timestamp"  # Set the parsed timestamp to the @timestamp field
#     }
     mutate {
      remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
     }
     if "_grokparsefailure" in [tags] {
       drop { }
     }
  }
  if [app] == "job-monitoring-db-log" { 
    grok {
      match => {
        "message" => "%{DATA:permissions}\s+%{NUMBER:hard_links}\s+%{USER:owner}\s+%{USER:group}\s+%{DATA:file_size}\s+(?<file_creation_timestamp>\b[A-Z][a-z]{2}\b\s+\d{1,2}\s+\d{2}:\d{2})\s+(?<file_name>.+\.sql\.gz)\s+(?<file_backup_date>\d{2}-\d{2}-\d{4})\s+(?<file_number>\d+)"
      }
    }
    date {
       match => [ "file_backup_date", "dd-MM-yyyy" ]
       target => "@timestamp"  # Set the parsed timestamp to the @timestamp field
    }
    if "_grokparsefailure" in [tags] {
       drop { }
    }
    mutate {
        remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
    }
  }
  if [app] == "job-monitoring-db-success" {
    grok {
      match => {
        "message" => "%{GREEDYDATA:log_message}"
      }
    }
    if "_grokparsefailure" in [tags] {
       drop { }
    }
    mutate {
        remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[original]"]
    }
  }
  if [app] == "job-monitoring-db-error" {
    grok {
      match => {
        "message" => "%{GREEDYDATA:log_message}"
      }
    }
    if "_grokparsefailure" in [tags] {
       drop { }
    }
    mutate {
        remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[original]"]
    }
  }
  if [app] == "job-monitoring-app-log" {
    grok {
      match => {
        "message" => "%{DATA:permissions}\s+%{NUMBER:hard_links}\s+%{USER:owner}\s+%{USER:group}\s+%{DATA:file_size}\s+(?<file_creation_timestamp>\b[A-Z][a-z]{2}\b\s+\d{1,2}\s+\d{2}:\d{2})\s+(?<file_name>.+\.sql\.gz)\s+(?<file_backup_date>\d{2}-\d{2}-\d{4})\s+(?<file_number>\d+)"
      }
    }
    date {
       match => [ "file_backup_date", "dd-MM-yyyy" ]
       target => "@timestamp"  # Set the parsed timestamp to the @timestamp field
    }
    if "_grokparsefailure" in [tags] {
       drop { }
    }
    mutate {
        remove_field => ["[@version]", "[message]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
    }
  }
  if [app] == "redis-monitoring-log" {
    grok {
      match => {
        "message" => [
                      "%{DATA:service_name}:%{DATA:process_name}\s+%{WORD:status}\s+pid\s+%{NUMBER:pid}\s*,\s*uptime\s+%{TIME:uptime}",
                      "%{DATA:service_name}:%{DATA:process_name}\s+%{WORD:status}\s+pid\s+%{NUMBER:pid},\s*uptime\s+%{NUMBER:uptime_days} day(?:s)?,\s*%{TIME:uptime_time}",
                      "%{DATA:service_name}:%{DATA:process_name}\s+%{WORD:status}\s+%{GREEDYDATA:log_time}"
         ]
      }
    }
    mutate {
      lowercase => ["status"]
    }
    if [status] == "running" {
      mutate {
        add_field => { "status_number" => 1 }
      }
    }
    else if [status] == "fatal" {
       mutate {
         add_field => { "status_number" => 2 }
       }
    }
    else if [status] == "stopped" {
       mutate {
         add_field => { "status_number" => 3 }
       }
    }
    mutate {
      convert => { "status_number" => "integer" }
    }
    mutate {
        remove_field => ["[@version]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[original]", "[input]"]
    }
  }

  if [app] == "synthetic-log" {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp}.*Starting processing for URL: %{DATA:url}\n.*Navigating to URL: %{DATA:navigated_url}\n.*URL: %{DATA:url_processing}\n.*Response Time: %{DATA:response_time}\n.*Response Time Status: %{GREEDYDATA:response_time_status}\n.*Expected Response Code: %{DATA:expected_response_code}\n.*Actual Response Code: %{DATA:actual_response_code}\n.*Response Code Status: %{GREEDYDATA:response_code_status}\n.*Expected Content Status: %{GREEDYDATA:expected_content_status}\n.*Finished processing for URL: %{DATA:url_processed}\n.*Email Status: %{GREEDYDATA:email_status}",
          "%{TIMESTAMP_ISO8601:timestamp}.*Starting processing for URL: %{DATA:url}\n.*ERROR - Error: %{DATA:error}\n.*Finished processing for URL: %{DATA:url_processed}\n.*Email Status: %{GREEDYDATA:email_status}"
        ]
      }
    }
    ruby {
      code => '
        expected_fields = {
          "response_time" => "N/A",
          "response_time_status" => "N/A",
          "expected_response_code" => "N/A",
          "actual_response_code" => "N/A",
          "response_code_status" => "N/A",
          "expected_content_status" => "N/A",
          "error" => "N/A"
        }
        expected_fields.each do |field, default_value|
          event.set(field, default_value) unless event.get(field)
        end
        # Check if email_status contains newline and truncate after N/A or Success
        email_status = event.get("email_status")
        if email_status && email_status.include?("\n")
          # Split at newline and keep only the first part
          event.set("email_status", email_status.split("\n").first)
        end
      '
    }
#    date {
#      match => [ "timestamp", "YYYY-MM-dd HH:mm:ss,SSS" ]
#      target => "@timestamp"
#    }
    mutate {
      add_field => { "formatted_log_timestamp" => "%{timestamp}" }
    }
    ruby {
      code => "
        # Parse the original log_timestamp string into a DateTime object
        original_timestamp = event.get('formatted_log_timestamp')
        require 'time'  # Require the time library for parsing
        parsed_time = Time.strptime(original_timestamp, '%Y-%m-%d %H:%M:%S,%L')  # Parse the original format
        # Format it to dd-MMM-YYYY hh:mm:ss AM/PM
        formatted_time = parsed_time.strftime('%d-%b-%Y %I:%M:%S %p')
        event.set('formatted_log_timestamp', formatted_time)  # Set the formatted time
      "
    }
    mutate {
      remove_field => ["[@version]", "[event]", "[agent]", "[ecs]", "[host]", "[log]", "[input]","[cloud]"]
    }
  }

}

output {
  if [app] == "app-frappe-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "app_frappe_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "app_frappe_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
    stdout {
      codec => rubydebug
    }
  }
  if [app] == "dr-sync-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "dr_sync_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "dr_sync_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
#    stdout {
#      codec => rubydebug
#    }
  }
  if [app] == "db-slow-queries-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "db_slow_queries_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "db_slow_queries_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
#    stdout {
#      codec => rubydebug
#    }
  }
#  if [app] == "db-slow-queries-test" {
#    elasticsearch {
#      hosts => ["localhost:9200"]
#      index => "db_slow_queries_test"
      #user => "elastic"
      #password => "Clover@123"
#    }
#    elasticsearch {
#      hosts => ["localhost:9200"]
#      index => "db_slow_queries_test-%{+dd-MM-YYYY}"
#    }
 #   stdout {
 #     codec => rubydebug
 #   }
#  }
  if [type] == "metricbeat_log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "metricbeat_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "metricbeat_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
    # stdout {
    #   codec => rubydebug
    # }
  }
  if [type] == "heartbeat_log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "heartbeat_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "heartbeat_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
    # stdout {
    #   codec => rubydebug
    # }
  }
  if [app] == "web-error-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "web_error_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "web_error_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
   # stdout {
   #    codec => rubydebug
   # }
  }
  if [app] == "app-nginx-error-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "app_nginx_error_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "app_nginx_error_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
    stdout {
       codec => rubydebug
    }
  }
  if [app] == "job-monitoring-db-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "job_monitoring_db_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "job_monitoring_db_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
    #stdout {
    #   codec => rubydebug
    #}
  }
  if [app] == "job-monitoring-db-success" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "job_monitoring_db_success_index"
      user => "elastic"
      password => "Clover@123"
    }
    #elasticsearch {
     # hosts => ["localhost:9200"]
     # index => "job_monitoring_db_index-%{+dd-MM-YYYY}"
     # user => "elastic"
     # password => "Clover@123"
    #}
    stdout {
       codec => rubydebug
    }
  }
  if [app] == "job-monitoring-db-error" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "job_monitoring_db_error_index"
      user => "elastic"
      password => "Clover@123"
    }
    #elasticsearch {
     # hosts => ["localhost:9200"]
     # index => "job_monitoring_db_index-%{+dd-MM-YYYY}"
    #}
    stdout {
       codec => rubydebug
    }
  }
  if [app] == "job-monitoring-app-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "job_monitoring_app_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "job_monitoring_app_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
   # stdout {
   #    codec => rubydebug
   # }
  }
  if [app] == "redis-monitoring-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "redis_monitoring_index"
      user => "elastic"
      password => "Clover@123"
    }
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "redis_monitoring_index-%{+dd-MM-YYYY}"
      user => "elastic"
      password => "Clover@123"
    }
    stdout {
       codec => rubydebug
    }
  }
  if [app] == "synthetic-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "synthetic_index"
      user => "elastic"
      password => "Clover@123"
    }
    stdout {
      codec => rubydebug
    }
  }
  if [type] == "uptime-log" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "uptime_index"
      user => "elastic"
      password => "Clover@123"
    }
    stdout {
      codec => rubydebug
    }
  }
#  stdout {
#    codec => rubydebug
#  }

}
